# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

# Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# %matplotlib inline

from sklearn import metrics
from sklearn import svm
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

dataset_path = 'pulsar_data_train.csv'

data = pd.read_csv(dataset_path)
data

"""# Exploratory Data Analysis - Deskripsi Variabel

Mengubah nama kolom menjadi lebih singkat
"""

data.columns = ['IP_mean', 'IP_std', 'IP_kurtosis', 'IP_skewness','DM-SNR_mean','DM-SNR_std', 'DM-SNR_kurtosis','DM-SNR_skewness', 'target']
data.info()

data.describe()

"""# Exploratory Data Analysis - Menangani Missing Value dan Outliers

## Menangani Missing Value

Dari data null yang didapatkan dari, dapat disimpulkan bahwa terdapat 3 kolom yang tidak memiliki nilai yaitu IP_kurtosis, DM-SNR_std, dan DM-SNR_skewness.
"""

data.isnull().sum()

"""Dan jika dilihat heatmap pada gambar dibawah, terlihat bahwa value yang null tersebar merata atau tidak fokus pada satu tempat."""

sns.heatmap(data.isnull())

"""Maka dari itu, untuk mengisi data yang null, saya memakai method fillna dengan method 'ffill', yang dimana method tersebut mengisi data yang kosong dengan cara mengambil data sebelumnya yang sudah terisi."""

data['IP_kurtosis'].fillna(method = "ffill", inplace = True)
data['DM-SNR_std'].fillna(method = "ffill", inplace = True)
data['DM-SNR_skewness'].fillna(method = "ffill", inplace = True)

data.isnull().sum()

sns.heatmap(data.isnull())

"""## Menangani Outliers

Menghitung jumlah outliers
"""

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
lower_range= Q1-(1.5 * IQR)
upper_range= Q3+(1.5 * IQR)
print('Total Outliers:')
((data < (lower_range)) | (data > (upper_range))).sum()

"""Menggambarkan data outliers dalam bentuk histogram dan boxplot. Dan dapat dilihat dari boxplot yang berada di sebelah kanan tersebut memiliki banyak outliers. Sementara untuk gambar Histogram, untuk melihat lebih jelas distribusi data.


"""

fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(15, 40))
fig.subplots_adjust(hspace = .4, wspace=.2)
for i in range(0,len(data.columns)-1):
  sns.histplot(data[data.columns[i]], ax=axes[i][0], kde=True).set_title("Hisotogram of " + data.columns[i],)
  sns.boxplot(x=data[data.columns[i]], ax=axes[i][1]).set_title("Boxplot " + data.columns[i])

"""Mengatasai outliers dengan menggunakan batas bawah dan batas atas dari data."""

def remove_outlier(col):
    sorted(col)
    Q1,Q3=np.percentile(col,[25,75])
    IQR=Q3-Q1
    lower_range= Q1-(1.5 * IQR)
    upper_range= Q3+(1.5 * IQR)
    return lower_range, upper_range

for column in data.loc[:,:'DM-SNR_skewness'].columns:
    if 1==1:
        lr,ur=remove_outlier(data[column])
        data[column]=np.where(data[column]>ur,ur,data[column])
        data[column]=np.where(data[column]<lr,lr,data[column])

"""Dan dapat dilihat dari hasil gambar tersebut, dari gambar boxplot dataset sudah tidak memiliki data outliers lagi, tetapi jika dilihat pada gambar histogram, terdapat satu jenis data yang sangat tinggi."""

fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(15, 40))
fig.subplots_adjust(hspace = .4, wspace=.2)
for i in range(0,len(data.columns)-1):
  sns.histplot(data[data.columns[i]], ax=axes[i][0], kde=True).set_title("Hisotogram of" + data.columns[i],)
  sns.boxplot(x=data[data.columns[i]], ax=axes[i][1]).set_title("Boxplot" + data.columns[i])

"""# Exploratory Data Analysis - Multivariate Analysis

Mengamati hubungan antar fitur numerik dengan fungsi pairplot, dan dapat dilihat dari gambar tersebut banyak data yang sangat terdistribusi, sehingga kita harus melihat dari sisi correlation matrix.
"""

sns.pairplot(data, diag_kind = 'kde')

"""Jika dilihat hasil dari correlation matrix pada gambar dibawah, dan jika fokus pada kolom target, setiap kolom yang berhubungan dengan target tidak ada nilainya yang mendekati nol, sehingga tidak perlu adanya penghapusan kolom.

"""

plt.figure(figsize=(10, 10))
correlation_matrix = data.corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix", size=20)

"""# Data Preparation

## Train-Test-Split
"""

X = data.drop(["target"],axis =1)
y = data["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Jumlah seluruh dataset: {len(X)}')
print(f'Jumlah train dataset: {len(X_train)}')
print(f'Jumlah test dataset: {len(X_test)}')

"""## Standarisasi"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_trains = scaler.fit_transform(X_train)
X_tests = scaler.transform(X_test)

"""# Model Development dengan SVM

Untuk metode development yang saya pakai adalah SVM, dikarenakan SVM cocok untuk data supervised learning yang memiliki dimensi tinggi dengan jumlah data yang banyak. Selain itu saya akan membandingkan kernel yang terdapat pada svm yaitu RBF kernel dan linier kernel.
"""

SVM = svm.SVC(random_state=1).fit(X_trains, y_train)

"""Sebelum menggunakan SVM, kita harus mencari parameter yang optimal untuk digunakan SVM."""

SVM_param_grid = {'C': [0.01,0.1, 1, 10],
              'gamma': [0.09, 0.1, 0.2, 0.001],
              'kernel': ['rbf'],
              'tol':[0.001,0.0001],
              'degree':[2,3]}

SVM_grid1 = GridSearchCV(SVM, param_grid = SVM_param_grid, cv = 5, n_jobs=-1)

SVM_grid1.fit(X_trains, y_train)

"""Setelah melakukan fit, kita mendapatkan hasil terbaiknya dengan memanggi method best_estimator"""

SVM_grid1.best_estimator_

"""## SVM dengan RBF Kernel

Lanjut dengan melakukan training. Dan dapat dilihat hasil klasifikasi metris sertaakurasi, presisi, dan recal yang dihasilkan oleh RBF kernel.
"""

SVM_grid_rbf = svm.SVC(C=10,degree=2, gamma=0.1, kernel='rbf', random_state=1)
SVM_grid_rbf.fit(X_trains, y_train)

"""## SVM dengan Linier Kernel

Setelah mencoba RBF kernel, selanjutnya mencoba menggunakan linier kernel, untuk tahap kali ini tidak memakai GridSearch karena sudah pernah dilakukan sebelumnya, dan untuk menbandingkan siapa yang terbaik, kita harus menyamakan parameter yang digunakan oleh RBF kernel.
"""

SVM_grid_linier = svm.SVC(C=10,degree=2, gamma=0.1, kernel='linear', random_state=1)
SVM_grid_linier.fit(X_trains, y_train)

"""# Evaluation

## SVM dengan RBF Kernel
"""

print('Hasil klasifikasi metriks dari training data:\n\n',metrics.classification_report(y_train, SVM_grid_rbf.predict(X_trains)),'\n')
print('Hasil klasifikasi metriks dari test data:\n\n',metrics.classification_report(y_test, SVM_grid_rbf.predict(X_tests)),'\n')

y_pred_rbf = SVM_grid_rbf.predict(X_tests)
print( "SVM best accuracy : " + str(np.round(metrics.accuracy_score(y_test,y_pred_rbf),3)*100)+'%')

print("Precision Score : ",np.round(precision_score(y_test, y_pred_rbf, pos_label='positive',average='micro'),3)*100,'%')
print("Recall Score : ",np.round(recall_score(y_test, y_pred_rbf, pos_label='positive',average='micro'),3)*100,'%')
print("F1-Score : ",np.round(f1_score(y_test, y_pred_rbf, pos_label='positive',average='micro'),3)*100,'%')

"""## SVM dengan Linier Kernel"""

print('Hasil klasifikasi metriks dari training data:\n\n',metrics.classification_report(y_train, SVM_grid_linier.predict(X_trains)),'\n')
print('Hasil klasifikasi metriks dari test data:\n\n',metrics.classification_report(y_test, SVM_grid_linier.predict(X_tests)),'\n')

y_pred_linier = SVM_grid_linier.predict(X_tests)
print( "SVM best accuracy : " + str(np.round(metrics.accuracy_score(y_test,y_pred_linier),4)*100)+'%')

print("Precision Score : ",np.round(precision_score(y_test, y_pred_linier, pos_label='positive',average='micro'),4)*100,'%')
print("Recall Score : ",np.round(recall_score(y_test, y_pred_linier, pos_label='positive',average='micro'),4)*100,'%')
print("F1-Score : ",np.round(f1_score(y_test, y_pred_linier, pos_label='positive',average='micro'),4)*100,'%')

"""**Dapat disimpulkan bahwa SVM dengan method yang paling optimal adalah RBF Kernel, walaupun hanya terdapat perbedaan sedikit**"""